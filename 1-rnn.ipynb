{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "continental-franchise",
   "metadata": {},
   "source": [
    "# LAB 9: Sentiment analysis using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-contribution",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-wound",
   "metadata": {},
   "source": [
    "Connect to the GPU (training RNNs without a GPU is veeery slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-matter",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"s3://ling583/sentiment.parquet\", storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"sentiment\"], random_state=619\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-canvas",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Because every problem and every model is a little bit different, pytorch (unlike scikit-learn) doesn't have built-in `fit` and `predict` methods. We need to define them ourselves here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-degree",
   "metadata": {},
   "source": [
    "This function gathers up a batch of training examples, encodes them, and sends them to the GPU for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    labels, texts = zip(*batch)\n",
    "    texts = [\n",
    "        [vocab[token] for token in [\"<s>\"] + tokenize(t) + [\"</s>\"]] for t in texts\n",
    "    ]\n",
    "    texts = [torch.tensor(t, dtype=torch.int64) for t in texts]\n",
    "    texts = pad_sequence(texts, padding_value=vocab[\"<pad>\"])\n",
    "    labels = torch.tensor([label_vocab[l] for l in labels], dtype=torch.int64)\n",
    "    return labels.to(device), texts.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-empty",
   "metadata": {},
   "source": [
    "This one applies the model to some test data, for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            results = []\n",
    "            for _, text in dataloader:\n",
    "                results.extend(model(text))\n",
    "    return results\n",
    "\n",
    "\n",
    "def predict(dataloader):\n",
    "    predicted = decision_function(dataloader)\n",
    "    return [label_vocab.itos[p.argmax()] for p in predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-truck",
   "metadata": {},
   "source": [
    "And this is the important part: the function that actually trains the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs=5, batch_size=64, wd=None, clip=None):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if wd:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), weight_decay=wd)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    t, v = train_test_split(train, test_size=0.1, stratify=train[\"sentiment\"])\n",
    "    train_dataset = list(zip(t[\"sentiment\"], t[\"text\"]))\n",
    "    valid_dataset = list(zip(v[\"sentiment\"], v[\"text\"]))\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        n = 0\n",
    "        for label, text in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                predicted = model(text)\n",
    "                loss = criterion(predicted, label)\n",
    "                correct += (predicted.argmax(1) == label).sum().item()\n",
    "                n += len(label)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        train_acc = correct / n * 100.0\n",
    "        valid_pred = predict(valid_dataloader)\n",
    "        valid_acc = accuracy_score(v[\"sentiment\"], valid_pred) * 100.0\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch:2d} Time: {elapsed:6.3f}s \"\n",
    "            f\"Train acc: {train_acc:5.3f} Valid acc: {valid_acc:5.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-differential",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Define the model class\n",
    "\n",
    "Okay, most of what's above is mostly [boilerplate](https://en.wikipedia.org/wiki/Boilerplate_code). Now we'll define the specific model and hyperparameter settings that we're using for this task.\n",
    "\n",
    "First, the model architecture. This is a basic [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network), using [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit)s are the recurrent units.\n",
    "\n",
    "The hyperparameters of interest are:\n",
    "\n",
    "* `hidden_size`\n",
    "* `embedding_size`\n",
    "* `hidden_layers`\n",
    "* `bidirectional`\n",
    "* `dropout`\n",
    "\n",
    "They control the ability of the model to learn details. Higher values for the first 3, plus setting `bidirectional` to `True`, increase the representational power of the model. That means it can learn more complex patterns and learn them more quickly. If these values are set too high, though, then the model can learn *too* well--it will simply memorize the training data and you'll get overfitting. The last value, dropout, helps control that. Higher values for `dropout` reduce the model's ability to learn and slow down training. The trick is finding a balance among all these settings that maximize learning while minimizing overfitting, which is unfortunately not easy to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab,\n",
    "        num_class,\n",
    "        hidden_size,\n",
    "        embedding_dim=128,\n",
    "        hidden_layers=1,\n",
    "        dropout=0.0,\n",
    "        bidirectional=True,\n",
    "    ):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        if not vocab.vectors is None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                vocab.vectors, freeze=True, padding_idx=vocab[\"<pad>\"]\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                len(vocab), embedding_dim, padding_idx=vocab[\"<pad>\"]\n",
    "            )\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embedding.embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=hidden_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        out_size = hidden_size * hidden_layers\n",
    "        if bidirectional:\n",
    "            out_size = out_size * 2\n",
    "        self.fc = nn.Linear(out_size, num_class)\n",
    "\n",
    "    def forward(self, text, lengths=None):\n",
    "        embedded = self.embedding(text)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        out = torch.cat(torch.unbind(hidden), axis=1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-motorcycle",
   "metadata": {},
   "source": [
    "Next we set up the vocabulary (this is the step performed by `CountVectorizer` in scikit-learn) using a [basic tokenizer](https://pytorch.org/text/stable/data_utils.html#get-tokenizer) that comes with pytorch. \n",
    "\n",
    "The \"specials\" are vocabulary items that don't correspond to words but are used internally by the model:\n",
    "\n",
    "* `<pad>` : For implementation reasons the documents in a batch all have to be the same length, so we add copies of the pseudo-word `<pad>` to the end of shorter reviews to make them as long as the longest one. \n",
    "* `<s>`, `</s>` : These mark the beginning and end of the reviews.\n",
    "* `<unk>` : Unknown words (i.e., words which are used in the test data that didn't get seen in the training data) get replaced with `<unk>`\n",
    "\n",
    "There's one adjustable parameter here: raising the value of `min_freq` removes low frequency lexical items (similar to `min_df` in scikit-learn). Increasing it usually doesn't improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = get_tokenizer(\"basic_english\")\n",
    "counter = Counter(concat(map(tokenize, tqdm(train[\"text\"]))))\n",
    "vocab = Vocab(\n",
    "    counter,\n",
    "    min_freq=1,\n",
    "    specials=(\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"),\n",
    ")\n",
    "label_vocab = Vocab(Counter(train[\"sentiment\"]), specials=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-equation",
   "metadata": {},
   "source": [
    "Now we instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(\n",
    "    vocab,\n",
    "    len(label_vocab),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=128,\n",
    "    hidden_layers=2,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-carol",
   "metadata": {},
   "source": [
    "And finally, we train! There are three important settings here:\n",
    "\n",
    "* `epochs` : This is the number of passes over the training data that we make when fitting the model. A crude way to avoid overfitting is to reduce this, which stops before training before the model has converged.\n",
    "* `batch_size` : This is the number of reviews that get processed at once during training. In general, increasing `batch_size` makes the program run faster (since it lets us take better advantage of the GPU) but may require more epochs to converge. Setting `batch_size` too high can overload the GPUs memory and lead to a crash. The effects of changing `batch_size` on the final results are hard to predict, but it can make a big difference.\n",
    "* `wd` : This is the \"[weight decay](https://www.fast.ai/2018/07/02/adam-weight-decay/)\" parameter. Setting this to a value other than `None` regularizes the model and can reduce overfitting (similar to setting `alpha` for `SGDClassifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs=5, batch_size=64, wd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = list(zip(test[\"sentiment\"], test[\"text\"]))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_predicted = predict(test_dataloader)\n",
    "acc = 100 * accuracy_score(test[\"sentiment\"], test_predicted)\n",
    "f1 = 100 * f1_score(test[\"sentiment\"], test_predicted, average=\"macro\")\n",
    "print(f\"Accuracy = {acc:.3f} F1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-religion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
